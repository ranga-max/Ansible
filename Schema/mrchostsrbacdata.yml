# Automatically created from terraform data on 

all:
  vars:
    ansible_connection: ssh
    ansible_user: ubuntu
    ansible_become: true
    installation_method: archive
    confluent_archive_file_source: /home/ubuntu/confluent-7.4.4.zip
    binary_base_path: /opt/confluent/confluent-7.4.4
    confluent_package_version: 7.4.4
    confluent_archive_file_remote: false
    #deployment_strategy: parallel
    deployment_strategy: rolling  
    ssl_enabled: true
    ssl_mutual_auth_enabled: false
    ssl_provided_keystore_and_truststore: true
   #ssl_keystore_filepath: "~/ssl/{{inventory_hostname}}-keystore.jks"
    ssl_keystore_filepath: "/home/ubuntu/Schema/certs/{{ inventory_hostname.split('.')|first }}.keystore.jks"
    ssl_keystore_key_password: confluent
    ssl_keystore_store_password: confluent
   #ssl_truststore_filepath: "~/ssl/kafka-truststore.jks"
    ssl_truststore_filepath: "/home/ubuntu/Schema/certs/{{ inventory_hostname.split('.')|first }}.truststore.jks"
    ssl_truststore_password: confluent
    ssl_truststore_ca_cert_alias: caroot 
    ssl_provided_keystore_and_truststore_remote_src: false  
      #validate_hosts: false  
    ansible_ssh_private_key_file: Week2_Trng.pem
    jolokia_enabled: false
    jmxexporter_enabled: true
    jmxexporter_url_remote: false
    jmxexporter_jar_url: /home/ubuntu/jmx_prometheus_javaagent-0.12.0.jar  
    rbac_enabled: true
    rbac_component_additional_system_admins:
      - rang
      - mds 
    #User:mds    
    #Centralized MDS Configuration    
    external_mds_enabled: true
    ## The URL for the MDS REST API on your Kafka Cluster hosting MDS
    mds_bootstrap_server_urls: https://kafka-2.rrchakdc1.ans.test.io:8090,https://kafka-3.rrchakdc1.ans.test.io:8090  
    mds_broker_bootstrap_servers: kafka-2.rrchakdc1.ans.test.io:9093,kafka-3.rrchakdc1.ans.test.io:9093
    mds_broker_listener:
       ssl_enabled: true
       ssl_mutual_auth_enabled: false  
       sasl_protocol: plain
    create_mds_certs: false
    #token_services_public_pem_file:
    #token_services_private_pem_file:
    #Centralized Audit Logging
    audit_logs_destination_enabled: true
    audit_logs_destination_bootstrap_servers: kafka-2.rrchakdc1.ans.test.io:9093,kafka-3.rrchakdc1.ans.test.io:9093
    audit_logs_destination_principal: "rang"  
    audit_logs_destination_kafka_cluster_name: "metriccluster"
    audit_logs_destination_listener:
        ssl_enabled: true
        ssl_mutual_auth_enabled: false
        sasl_protocol: plain
    ##  
    kafka_broker_cluster_name: tenantcluster
    schema_registry_cluster_name: tenantcluster-sr
    schema_registry_cluster_id: tenantcluster-sr  
    kafka_connect_cluster_name: tenantcluster
    ksql_cluster_name: tenantcluster-connect
    #secrets_protection_enabled: true
    confluent_cli_download_enabled: false   
    #secrets_protection_masterkey: abC12DE+3fG45Hi67J8KlmnOpQr9s0Tuv+w1x2y3zab=
    #rbac_component_additional_system_admins: [ user1 ]
    mds_super_user: mds
    mds_super_user_password: mds-secret
    kafka_broker_ldap_user: kafka
    kafka_broker_ldap_password: kafka-secret
    schema_registry_ldap_user: sr
    schema_registry_ldap_password: sr-secret
    kafka_connect_ldap_user: connect
    kafka_connect_ldap_password: connect-secret
    ksql_ldap_user: ksql
    ksql_ldap_password: ksql-secret
    kafka_rest_ldap_user: rest
    kafka_rest_ldap_password: rest-secret
    control_center_ldap_user: c3
    control_center_ldap_password: c3-secret  
    sasl_protocol: plain
    kafka_broker_custom_listeners:
        broker:
            name: BROKER
            port: 9091
            ssl_enabled: true
            ssl_mutual_auth_enabled: false  
        internal:
            name: INTERNAL
            port: 9092
            ssl_enabled: true
            ssl_mutual_auth_enabled: false  
        client_listener:
            name: CLIENT
            port: 9093
            ssl_enabled: true
            ssl_mutual_auth_enabled: false  
    sasl_plain_users:
       admin:
        principal: rang
        password: rang-secret      
    kafka_broker_custom_properties:
        auto.create.topics.enable: false
        num.partitions: 1
        min.insync.replicas: 1
        default.replication.factor: 1
        offsets.topic.replication.factor: 1
        #num.partitions: 4
        #min.insync.replicas: 3
        #default.replication.factor: 6
        #offsets.topic.replication.factor: 6  
        log.retention.ms: 43200000
        #authorizer.class.name: "kafka.security.authorizer.AclAuthorizer"
        authorizer.class.name: "io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer"
        allow.everyone.if.no.acl.found: "true"
        super.users: "User:kafka;User:sr;User:c3;User:rest"
        replica.selector.class: org.apache.kafka.common.replica.RackAwareReplicaSelector
        confluent.balancer.enable: true
        confluent.balancer.heal.uneven.load.trigger: ANY_UNEVEN_LOAD
        confluent.balancer.heal.broker.failure.threshold.ms: 360000
        confluent.log.placement.constraints: '{"version": 2,"replicas": [{"count": 1,"constraints": {"rack": "us-east-1a-data"
}},{"count": 1,"constraints": {"rack": "us-east-1b-data"}}]}'  
        #confluent.log.placement.constraints: '{"version": 2,"replicas": [{"count": 2,"constraints": {"rack": "us-east-1a"
  #}},{"count": 2,"constraints": {"rack": "us-east-1b"}}],"observers": [{"count": 1,"constraints": {"rack": "us-east-1a"}},
  #{"count": 1,"constraints": {"rack": "us-east-1b"}}],"observerPromotionPolicy":"under-min-isr"}'
  #confluent.offsets.topic.placement.constraints: '{"version": 2,"replicas": [{"count": 2,"constraints": {"rack": "us-east-1a"
  #}},{"count": 2,"constraints": {"rack": "us-east-1b"}}],"observers": [{"count": 1,"constraints": {"rack": "us-east-1a"}},
  #{"count": 1,"constraints": {"rack": "us-east-1b"}}],"observerPromotionPolicy":"under-min-isr"}'
  #confluent.transaction.state.log.placement.constraints: '{"version": 2,"replicas": [{"count": 2,"constraints": {"rack": "us-east-1a"
  #}},{"count": 2,"constraints": {"rack": "us-east-1b"}}],"observers": [{"count": 1,"constraints": {"rack": "us-east-1a"}},
  #{"count": 1,"constraints": {"rack": "us-east-1b"}}],"observerPromotionPolicy":"under-min-isr"}'
        ldap.java.naming.factory.initial: com.sun.jndi.ldap.LdapCtxFactory
        ldap.com.sun.jndi.ldap.read.timeout: 3000
          # ldap.java.naming.provider.url: "ldaps://openldap.ans.test.io:636"
        ldap.java.naming.provider.url: "ldap://openldap.ans.test.io:391"
          #ldap.java.naming.security.protocol: SSL
          #ldap.ssl.truststore.location: "{{kafka_broker_truststore_path}}"
          #ldap.ssl.truststore.location: "/home/ubuntu/ldap_ssl/kafka_broker.truststore.jks" 
          #ldap.ssl.truststore.password: "{{kafka_broker_truststore_storepass}}"
          #ldap.ssl.truststore.password: changeit  
        #ldap.java.naming.security.principal: alice@.CONFLUENT.IiO
        ldap.java.naming.security.principal: "cn=admin,dc=confluent,dc=io"
        #ldap.java.naming.security.credentials: alice-secret
        ldap.java.naming.security.credentials: confluent
        ldap.java.naming.security.authentication: simple
        ldap.user.search.base: dc=confluent,dc=io
          #ldap.user.search.base: dc=confluent,dc=io 
        # Required for group-based search
        ldap.group.search.base: dc=confluent,dc=io
        ldap.group.object.class: posixGroup
        ldap.group.name.attribute: cn
        ldap.group.member.attribute: member
        ldap.group.member.attribute.pattern: CN=(.*),dc=confluent,dc=io
        #group based search ends
        ldap.search.mode: USERS
        #ldap.search.mode: GROUPS  
        #ldap.user.password.attribute: userpassword
        #ldap.user.name.attribute: sAMAccountName
        #ldap.user.name.attribute: givenName
        ldap.user.name.attribute: cn  
        ldap.user.memberof.attribute: memberOf
        ldap.user.memberof.attribute.pattern: CN=(.*),dc=confluent,dc=io
        ldap.user.object.class: inetOrgPerson
        ldap.user.search.scope: 2
        ldap.authorizer.ssl.truststore.location: "{{ kafka_broker_truststore_path }}"
        ldap.authorizer.ssl.truststore.password:  "{{ kafka_broker_truststore_storepass }}"
        confluent.basic.auth.credentials.source: USER_INFO
        confluent.basic.auth.user.info: "sr:sr-secret"
        # Special callback for ldap listener
        listener.name.client.plain.sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required;
        listener.name.client.plain.sasl.server.callback.handler.class: io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler
        #listener.name.ldap.plain.sasl.server.callback.handler.class: io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler
        listener.name.client.ssl.principal.mapping.rules: |
            RULE:^CN=([a-zA-Z0-9]*).*$/$1/L ,\
            DEFAULT
        #Centralized Metrics Reporting Configuraiton; No Control Center Component defined for data cluster   
        kafka.broker.metrics.reporter.enabled: true
        metric.reporters: io.confluent.metrics.reporter.ConfluentMetricsReporter  
        confluent.metrics.reporter.bootstrap.servers: kafka-2.rrchakdc1.ans.test.io:9093,kafka-3.rrchakdc1.ans.test.io:9093
        confluent.metrics.reporter.security.protocol: SASL_SSL
        confluent.metrics.reporter.sasl.mechanism: PLAIN
        confluent.metrics.reporter.sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username="rang" password="rang-secret";
        confluent.metrics.reporter.ssl.truststore.location: "{{ kafka_broker_truststore_path }}" 
        confluent.metrics.reporter.ssl.truststore.password: "{{ kafka_broker_truststore_storepass }}"
          
 


zookeeper:
  hosts:
    #zookeeper-0.rrchakdc1.ans.test.io:
    kafka-0.rrchakdc1.ans.test.io:  
       zookeeper_id: 1
    #kafka-0.rrchakdc2.ans.test.io:
    #   zookeeper_id: 2
    #kafka-1.rrchakdc2.ans.test.io:
    #   zookeeper_id: 3

kafka_broker:
  hosts:
    kafka-0.rrchakdc1.ans.test.io:
       broker_id: 1
       kafka_broker_custom_properties:
         broker.rack: us-east-1a-data
    kafka-1.rrchakdc1.ans.test.io:
       broker_id: 2
       kafka_broker_custom_properties:
         broker.rack: us-east-1b-data
  # kafka-2.rrchakdc1.ans.test.io:
  #     broker_id: 3
  #     kafka_broker_custom_properties:
  #       broker.rack: us-east-1a
 #  kafka-0.rrchakdc2.ans.test.io:
 #     broker_id: 4
 #     kafka_broker_custom_properties:
 #        broker.rack: us-east-1b
#   kafka-1.rrchakdc2.ans.test.io:
  #    broker_id: 5
  #     kafka_broker_custom_properties:
  #       broker.rack: us-east-1b
#  kafka-2.rrchakdc2.ans.test.io:   
#       broker_id: 6
#       kafka_broker_custom_properties:
#         broker.rack: us-east-1b

schema_registry:
  #vars:
  #  schema_registry_group: cgi_sr_grp
  #   schema_registry_custom_properties:
  #     kafkastore.topic: _schemas_mrc1
  #     leader.eligibility: true
  #     schema.registry.group.id: 1
      #host.name: localhost  
  hosts:
      # schema_registry_custom_properties:
      # leader.eligibility: false
    #kafka-1.rrchakdc2.ans.test.io:  
      #schema_registry_custom_properties:
      # leader.eligibility: false
    #schema-0.rrchakdc1.ans.test.io:
    #schema-1.rrchakdc1.ans.test.io:  
    kafka-0.rrchakdc1.ans.test.io:
    kafka-1.rrchakdc1.ans.test.io:  
       #schema_registry_custom_properties:
         #  leader.eligibility: true
         #  host.name: kafka-0.rrchakdc1.ans.test.io


#control_center:
#  vars:
#    control_center_custom_properties:
#       confluent.controlcenter.id: 1
#       confluent.controlcenter.command.topic: _confluent_command_1
#       confluent.controlcenter.internal.topics.retention.ms: 14400000
#       confluent.metrics.topic.retention.ms: 14400000
#       confluent.monitoring.interceptor.topic.retention.ms: 14400000   
#  hosts:
    #controlcenter-0.rrchakdc1.ans.test.io:
#    kafka-0.rrchakdc1.ans.test.io: 

#kafka_rest:
  ## To set variables on all kafka_rest hosts, use the vars block here
##  vars:
#   kafka_rest_custom_properties:
#      kafka.rest.authentication.method: BASIC
#      kafka.rest.authentication.realm: KafkaRest
  #   ## To configure Rest Proxy to run as a custom user, uncomment below
  #   kafka_rest_user: custom-user
  #   kafka_rest_group: custom-group
  #
  #   ## To copy files to kafka_rest hosts, set this list below
  #   kafka_rest_copy_files:
  #     - source_path: /path/to/file.txt
  #       destination_path: /tmp/file.txt
  # hosts:
  #  kafka-2.rrchakdc1.ans.test.io:
